# Distributional Offline Continuous-Time Reinforcement Learning with Neural Physics-Informed PDEs

![DOCTRL_pres_image-2](https://github.com/user-attachments/assets/7ed7a90a-42e0-4ac2-b7d7-e95779c9981a)


This repository provides a full implementation of the **DEEP DOCTR-L** algorithm, based on:

**“Distributional Offline Continuous-Time Reinforcement Learning with Neural Physics-Informed PDEs” – Igor Halperin (2023)**

It was developed as part of the course *Machine Learning and Stochastic Control* in the Master **Probabilités et Finance** at Sorbonne Université.

The repository includes:
- The complete code to reproduce the original experiments described in the paper
- A main script to run the full pipeline: data generation, model training, and evaluation

## 📄 Resources

- **[Original Paper](paper/Halperin_2023_DOCTR-L.pdf)**
- **[Project Report](report/DOCTR-L_Report.pdf)** — presents the method, key derivations, and additional experiments comparing DOCTR-L to a semi-closed-form Riccati solution

> 🔧 The Riccati-based experiments from the report are **not included** in this repository.
